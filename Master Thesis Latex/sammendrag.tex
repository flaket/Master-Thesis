%%=========================================
%\addcontentsline{toc}{section}{Sammendrag \& konklusjoner}
\section*{Sammendrag og konklusjoner}
Dette prosjektet har hatt som mål å utforske ulike brukergrensesnitt i smarte hjem. Spesielt har fokuset vært på å utforske \emph{intelligente brukergrensesnitt}; alternative interaksjonsmetoder som kan ha økt effektivitet og som oppleves som mer naturlige i bruk. Dette ble utfordret gjennom å spørre hva slags programvare man ønsker i smarte hjem. Basert på logisk argumentasjon og empiriske bevis fra to studier, viste jeg at brukere av smarte hjem ønsker å lære om hjemmets tilstand, og de ønsker å utøve styring over hjemmet. Samtidig må privatliv og personvern beskyttes. Med dette utgangspunktet utforsket jeg hvordan gester og tale kan benyttes for å gi kommandoer, og hvordan informasjonen om hjemmets tilstand best mulig kan presenteres.

Jeg har vist at maskinlæring kan benyttes for å gi enkle sensorer en mer detaljert forståelse av abstrakte gester, enn det som eksplisitt kan programmeres. På bakgrunn av teknologiske begrensninger og sikkerhetsmessige problemer, har jeg vist at det i et hjem-scenario er mer aktuelt med begrenset talegjenkjenning enn naturlig talegjenkjenning. Jeg har vist at begrenset talegjenkjenning og maskinlærte gester i kombinasjon er en aktuell, multimodal måte å styre hjemmet på. Til sist har jeg vist hvordan et tradisjonelt brukergrensesnitt kan forbedres ved å designe rundt presentasjon av kontekstinformasjonen, framfor brukerinteraksjon. En prototype ble utviklet, der fokusert var å så mye informasjon som mulig om hjemmet gjennom teknikker fra grafisk design. Regler for hvordan å vise den mest essensielle informasjonen ble utforsket og implementert i et grafisk brukergrensesnitt, som forandrer seg dynamisk basert på kontinuerlige datastrømmer fra det smarte hjemmet.