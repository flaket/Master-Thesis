%%=========================================
%\addcontentsline{toc}{section}{Sammendrag \& konklusjoner}
\section*{Abstract}
The goal of this thesis has been to explore user interfaces with regards to smart homes. In particular, intelligent user interfaces has been of focus; alternate methods of interaction, which might prove more efficient, effective and natural. This was challenged by asking what sort of software is of need in smart homes. Based on logical argumentation and empirical evidence from two case studies, I showed that users of smart homes wishes to learn about the state of the home, as well as having the ability to excise control over the home. Simultaniously, these features must not create issues regarding privacy or security. With this starting point I explored how gestures and speech could be used to issue commands, and how information regarding the state of the home could be presented. If users are interested in learning about the home they should be given information software; software which prioritizes data, not user interaction. The focus has been to show as much data as possible in an optimal way, through techniques from graphical design. Rules for deciding which information is the most essential to present was explored and implemented in a graphical user interface, which changes dynamically based on continous data streams from the smart home.

I have shown that machine learning can be used to give simple sensors a more detailed understanding of abstract gestures, than what can be explicitly programmed. Based on technological and security-related issues I have shown that limited speech recognition is more appropriate in home scenarios, than natural speech recognition. I have shown that the combination of limited speech recognition and machine learned gestures, is a possible, multimodal approach of controlling the home. Finally, I have shown that a more traditional user interface can be improved by designing around the presentation of context information, rather than user interaction.